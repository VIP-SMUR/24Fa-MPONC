{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:58:24.317365Z",
     "start_time": "2024-10-21T00:58:20.718614Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib.colors import Normalize\n",
    "import requests\n",
    "import zipfile\n",
    "import itertools\n",
    "\n",
    "# #4-step model:\n",
    "# import pandas\n",
    "# import geopandas\n",
    "# import json\n",
    "# import math\n",
    "# from haversine import haversine\n",
    "# from ipfn import ipfn\n",
    "# import networkx\n",
    "# from matplotlib import pyplot\n",
    "# from matplotlib import patheffects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# CONFIGURATION\n",
    "# =============\n",
    "\n",
    "# URL of the file to download\n",
    "url = \"https://services1.arcgis.com/Ug5xGQbHsD8zuZzM/arcgis/rest/services/ACS_2022_Geographic_boundaries/FeatureServer/replicafilescache/ACS_2022_Geographic_boundaries_-7361254879251475346.zip\"\n",
    "\n",
    "# Name of zip folder\n",
    "filename = \"ACS_2022_Geographic_boundaries_-7361254879251475346.zip\"\n",
    "# Name of extracted folder (same as above, excluding '.zip')\n",
    "extracted_name = filename.rsplit('.', 1)[0] \n",
    "\n",
    "# Format: ([GEOID], [IS_IN_BELTLINE])\n",
    "GEOIDS = [\n",
    "    ('BELTLINE01', True),\n",
    "    ('BELTLINE02', True),\n",
    "    ('BELTLINE03', True),\n",
    "    ('BELTLINE04', True),\n",
    "    ('BELTLINE05', True),\n",
    "    ('BELTLINE06', True),\n",
    "    ('BELTLINE07', True),\n",
    "    ('BELTLINE08', True),\n",
    "    ('BELTLINE09', True),\n",
    "    ('BELTLINE10', True),\n",
    "    ('HS6443070', False),   # Gresham Park\n",
    "    ('HS6442055', False),   # Druid Hills\n",
    "    ('1322052', False),     # Decatur city\n",
    "    ('1325720', False),     # East Point\n",
    "    ('HS6331054', False),   # Campbell HS\n",
    "    ('1310944', False),     # BrookHaven City\n",
    "    ('HS6440172', False),   # Panthersville\n",
    "    \n",
    "    ('1304000', False)      # Atlanta City - KEEP AS LAST ELEMENT (for graphing purposes)\n",
    "    ]\n",
    "\n",
    "EPSILON = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T01:53:16.022049Z",
     "start_time": "2024-10-21T01:53:13.526784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file already exists. Skipping download.\n",
      "Extraction folder already exists. Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# DOWNLOAD AND EXTRACT ZIP\n",
    "# ========================\n",
    "\n",
    "# Path to current directory\n",
    "cwd = Path.cwd()\n",
    "# Path to extraction folder\n",
    "extracted_path = cwd / 'data' / extracted_name\n",
    "\n",
    "def download_file(url, filename):\n",
    "    # Use a 'data' subfolder in the current working directory\n",
    "    data_dir = cwd / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)  # Create the 'data' directory if it doesn't exist\n",
    "    file_path = data_dir / filename\n",
    "\n",
    "    # Check if ZIP file already exists\n",
    "    if file_path.exists():\n",
    "        print(f\"ZIP file already exists. Skipping download.\")\n",
    "        return file_path\n",
    "        \n",
    "    else:\n",
    "        # Make the request\n",
    "        print(f\"Downloading {filename} to {data_dir}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'application/zip' not in content_type and 'application/octet-stream' not in content_type:\n",
    "                print(f\"Unexpected Content-Type: {content_type}. The downloaded file may not be a ZIP archive.\")\n",
    "                return None\n",
    "            \n",
    "            # Get the total file size\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            # Open the file and use tqdm for the progress bar\n",
    "            with file_path.open('wb') as file, tqdm(\n",
    "                desc='Downloading...',\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as progress_bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    progress_bar.update(size)\n",
    "            print(f\"Successfully downloaded {filename}\\n\")\n",
    "            return file_path  # Return the file path after successful download\n",
    "                \n",
    "        else:\n",
    "            print(f\"Failed to download {filename}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "def extract_file(file_path):\n",
    "    if not zipfile.is_zipfile(file_path):\n",
    "        print(f\"The file {file_path} is not a valid ZIP archive. Extraction aborted.\")\n",
    "        return\n",
    "    \n",
    "    data_dir = file_path.parent\n",
    "    extract_path = data_dir / extracted_name # Get path to unzipped extract folder\n",
    "    \n",
    "    if extracted_path.exists():\n",
    "        print(f\"Extraction folder already exists. Skipping extraction.\")\n",
    "        \n",
    "    else:\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            \n",
    "            # Extract the ZIP file\n",
    "            extract_path.mkdir(exist_ok=True) # Create the extraction folder if it doesn't exist\n",
    "            print(f\"Extracting {file_path.name} to {extract_path}...\")\n",
    "            \n",
    "            # Get the total number of files in the ZIP\n",
    "            total_files = len(zip_ref.infolist())\n",
    "            # Use tqdm for the extraction progress bar\n",
    "            for file in tqdm(zip_ref.infolist(), desc=\"Extracting\", total=total_files):\n",
    "                zip_ref.extract(file, extract_path)\n",
    "        \n",
    "        print(f\"Successfully extracted {file_path.name} to {extract_path}\")\n",
    "        \n",
    "# Download file\n",
    "file_path = download_file(url, filename)\n",
    "\n",
    "#Extract file\n",
    "if file_path:\n",
    "    extract_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached GA_gdf file.\n",
      "Loading GeoDataFrame from cache: c:\\Users\\kmmat\\OneDrive\\Desktop\\VIP\\24Fa-MPONC\\modeling_processes_of_neighborhood_change_new\\data\\GA_gdf.gpkg\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# GDF FILE INITIALIZATION\n",
    "# =======================\n",
    "\n",
    "# Establish/create paths\n",
    "figures_folder = cwd / \"figures\" # Path to figures folder\n",
    "figures_folder.mkdir(parents=True, exist_ok=True) # Create if DNE\n",
    "\n",
    "# Name of cached GDF file\n",
    "GA_gdf_cache_file_name = \"GA_gdf.gpkg\"\n",
    "# Path to cached GDF file\n",
    "GA_gdf_cache_file = cwd / 'data' / GA_gdf_cache_file_name\n",
    "\n",
    "\n",
    "def load_gdf(cache_file):\n",
    "    print(f\"Loading GeoDataFrame from cache: {cache_file}\")\n",
    "    GA_gdf = gpd.read_file(cache_file)\n",
    "    return GA_gdf\n",
    "    \n",
    "def create_gdf():\n",
    "    print(f\"Initializing GDF file for the first time: {shapefile_path}\")\n",
    "    GA_gdf = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    # Simplify geometries\n",
    "    GA_gdf['geometry'] = GA_gdf['geometry'].simplify(tolerance=0.001, preserve_topology=True)    \n",
    "    \n",
    "    # Add 'Sqkm' area column:\n",
    "    GA_gdf = GA_gdf.to_crs(epsg=32616)  # Update CRS for area calculations\n",
    "    GA_gdf['Sqkm'] = GA_gdf['geometry'].area / 1e+6 \n",
    "    \n",
    "    # Update CRS for general calculations (ex. amenity retrieval)\n",
    "    GA_gdf = GA_gdf.to_crs(epsg=4326)\n",
    "    \n",
    "    # Save to cache\n",
    "    print(f\"Saving processed GeoDataFrame to cache: {GA_gdf_cache_file}\")\n",
    "    GA_gdf.to_file(GA_gdf_cache_file, driver='GPKG')  # Using GeoPackage for better compatibility\n",
    "    \n",
    "    return GA_gdf\n",
    "\n",
    "\n",
    "# ====================\n",
    "# MAIN EXECUTION LOGIC\n",
    "# ====================\n",
    "if GA_gdf_cache_file.exists():\n",
    "    print(f\"Using cached GA_gdf file.\")\n",
    "    GA_gdf = load_gdf(GA_gdf_cache_file)\n",
    "else:\n",
    "    shapefile_path = extracted_path / 'Geographic_boundaries,_ACS_2022.shp'  # Define shapefile path\n",
    "    GA_gdf = create_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded from c:\\Users\\kmmat\\OneDrive\\Desktop\\VIP\\24Fa-MPONC\\modeling_processes_of_neighborhood_change_new\\data\\GA_graph.pkl.\n",
      "Graph already exists. Using cached graph.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# GRAPH FILE INITIALIZATION\n",
    "# =========================\n",
    "\n",
    "# Name of pickled graph file\n",
    "graph_file_name = \"GA_graph.pkl\"\n",
    "# Path to pickled graph file\n",
    "graph_file = cwd / 'data' / graph_file_name\n",
    "\n",
    "# Only account for valid GEOID's for graph generation\n",
    "valid_GEOIDS = set(GA_gdf['GEOID']) # Set of valid GEOID's\n",
    "used_GEOIDS = [] # Array to store (valid) GEOID's being used for simulation\n",
    "invalid_GEOIDS = [] # Array to store invalid GEOID's\n",
    "gdf_sub_array = [] # Array containing each GEOID's gdf\n",
    "\n",
    "def create_graph():\n",
    "    for geoid, _ in tqdm(GEOIDS, desc=\"Filtering for GEOIDs\", unit=\"geoid\"):\n",
    "        if geoid in valid_GEOIDS:\n",
    "            # Filter GA_gdf for the current GEOID\n",
    "            gdf_sub = GA_gdf[GA_gdf['GEOID'] == geoid]\n",
    "            gdf_sub_array.append(gdf_sub)\n",
    "            used_GEOIDS.append(geoid)\n",
    "        else:\n",
    "            invalid_GEOIDS.append(geoid)\n",
    "            \n",
    "    # Concatenate GEOID regions\n",
    "    if (gdf_sub_array):\n",
    "        gdf_combined = pd.concat(gdf_sub_array, ignore_index=True)\n",
    "    else:\n",
    "        raise ValueError(\"No valid GEOID's found.\")\n",
    "    \n",
    "    # Combines all polygons within GEOID regions\n",
    "    combined_shape = gdf_combined.geometry.union_all() \n",
    "    \n",
    "    print(\"Generating graph from OSMnx...\")\n",
    "    \n",
    "    g = ox.graph_from_polygon(combined_shape, network_type='drive', simplify=True) #Roadmap of GA (networkx.MultiDiGraph)\n",
    "    g = g.subgraph(max(nx.strongly_connected_components(g), key=len)).copy() #Ensures all nodes are connected\n",
    "    g = nx.convert_node_labels_to_integers(g) #Converts nodes to integers\n",
    "    \n",
    "    print(\"Graph generated.\")\n",
    "    if len(invalid_GEOIDS) > 0:\n",
    "        print (f\"Invalid GEOID's: {invalid_GEOIDS}\")\n",
    "    return g, used_GEOIDS\n",
    "   \n",
    "def save_graph(g, used_GEOIDS, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump({'graph': g, 'GEOIDS': used_GEOIDS}, file)\n",
    "    print(f\"Graph saved to {file_path}.\")\n",
    "    \n",
    "def load_graph(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    g = data['graph']\n",
    "    saved_GEOIDS = data['GEOIDS']\n",
    "    print(f\"Graph loaded from {file_path}.\")\n",
    "    return g, saved_GEOIDS\n",
    "\n",
    "\n",
    "# ====================\n",
    "# MAIN EXECUTION LOGIC\n",
    "# ====================\n",
    "if graph_file.exists():\n",
    "    # Load existing graph and its GEOIDS\n",
    "    g, saved_GEOIDS = load_graph(graph_file)\n",
    "    \n",
    "    # Populate used_GEOIDS based on current valid GEOIDs in GA_gdf\n",
    "    used_GEOIDS = [geoid for geoid, _ in GEOIDS if geoid in set(GA_gdf['GEOID'])]\n",
    "    \n",
    "    # Compare saved_GEOIDS with current GEOIDS\n",
    "    if set(saved_GEOIDS) != set(used_GEOIDS):\n",
    "        print(\"GEOID's have changed. Recreating the graph...\")\n",
    "        g, used_geoids = create_graph()\n",
    "        save_graph(g, used_GEOIDS, graph_file)\n",
    "    else:\n",
    "        print(\"Graph already exists. Using cached graph.\")\n",
    "        \n",
    "else:\n",
    "    # Create new graph\n",
    "    print (\"Creating graph for the first time.\")\n",
    "    g, used_GEOIDS = create_graph()\n",
    "    save_graph(g, used_GEOIDS, graph_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Centroids: 100%|██████████| 17/17 [00:00<00:00, 1096.36centroid/s]\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# CENTROID INITIALIZATION\n",
    "# =======================\n",
    "\n",
    "#CENTROIDS\n",
    "centroids = []\n",
    "# tuple format: (longitude, latitude, region_name, is_beltline)\n",
    "\n",
    "GEOID_info = {geoid: is_beltline for geoid, is_beltline in GEOIDS}\n",
    "\n",
    "# CENTROID INITIALIZATION FROIM GEOIDS\n",
    "for geoid in tqdm(used_GEOIDS[:-1], desc=\"Initializing Centroids\", unit=\"centroid\"):\n",
    "    \n",
    "    # Default is_beltline to False if it's not in tuple\n",
    "    is_beltline = GEOID_info.get(geoid, False)\n",
    "    \n",
    "    # Fetch GEOID instance from GA_gdf\n",
    "    gdf_sub = GA_gdf[GA_gdf['GEOID'] == geoid]\n",
    "    \n",
    "    # Combined geometry of all geometries in gdf_sub\n",
    "    combined_geometry = gdf_sub.geometry.union_all()\n",
    "    \n",
    "    # Initialize centroid with coordinates\n",
    "    centroid = combined_geometry.centroid\n",
    "    centroids.append((centroid.x, centroid.y, gdf_sub['Name'].iloc[0], is_beltline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing amenity densities: 100%|██████████| 17/17 [00:00<00:00, 20.43GEOID/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Region]                                 [Area (sq km)] [# Amenities]\n",
      "RDA/Lee/Cascade/Enota Park                       8.11        5.0\n",
      "Heritage Communities of South Atlanta            7.35        0.0\n",
      "Boulevard/McDonough/Boulevard Crossing           5.05        5.0\n",
      "Memorial/Glenwood/Grant Park                     4.42        0.0\n",
      "Freedom Parkway/Fourth Ward Park                 4.61        4.0\n",
      "Virginia-Highland/Ansley/Piedmont Park           6.82        3.0\n",
      "Peachtree/Collier/Tanyard Creek                  8.47        7.0\n",
      "Howell/Northside/Water Works                     6.67        3.0\n",
      "West Marietta/Howell/Westside Park               5.65        1.0\n",
      "Hollowell/Boone/Maddox Park                      5.39        1.0\n",
      "McNair, Ronald E. HS                            41.17        1.0\n",
      "Druid Hills HS                                  45.92       49.0\n",
      "Decatur city                                    11.93       16.0\n",
      "East Point city                                 38.10       13.0\n",
      "Campbell HS                                     85.21       67.0\n",
      "Brookhaven city                                 32.09        6.0\n",
      "Cedar Grove HS                                  39.51        0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing centroid distances:   0%|          | 0/17 [00:00<?, ?it/s]\n",
      "Computing centroid distances: 100%|██████████| 17/17 [00:00<00:00, 93573.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# AMENITY DENSITY & CENTROID DISTANCE CALCULATIONS\n",
    "# ================================================\n",
    "\n",
    "# VIEW REGION AREAS AND # AMENITIES?\n",
    "viewData = True\n",
    "data_output = []\n",
    "\n",
    "# Amenity Density Calculation\n",
    "def compute_amts_dens():\n",
    "    \n",
    "    # [AMENITY FILTER]\n",
    "    tags = {'highway':'bus_stop'} #TODO: I don't think number of bus stops is accurate\n",
    "\n",
    "    # Initialize empty arrays\n",
    "    amenities = np.zeros(len(used_GEOIDS) - 1)\n",
    "    areas_sqkm = np.zeros(len(used_GEOIDS) - 1)\n",
    "        \n",
    "    # Iterate over each GEOID\n",
    "    for index, geoid in enumerate(tqdm(used_GEOIDS[:-1], desc=\"Computing amenity densities\", unit=\"GEOID\")):\n",
    "        \n",
    "        name = GA_gdf.loc[GA_gdf['GEOID'] == geoid, 'Name'].iloc[0]\n",
    "    \n",
    "        # Extract polygon of current GEOID\n",
    "        polygon = GA_gdf.loc[GA_gdf['GEOID'] == geoid, 'geometry'].union_all()\n",
    "        \n",
    "        # Collect amenities\n",
    "        try:\n",
    "            amenities[index] = len(ox.features_from_polygon(polygon, tags))\n",
    "            \n",
    "        except: # No bus stops\n",
    "            amenities[index] = 0\n",
    "        \n",
    "        # Area of current GEOID\n",
    "        areas_sqkm[index] = GA_gdf.loc[GA_gdf['GEOID'] == geoid, 'Sqkm'].iloc[0]\n",
    "        \n",
    "        if (viewData):\n",
    "            data_output.append(f\"{name:<40} {areas_sqkm[index]:>12.2f} {amenities[index]:>10}\")\n",
    "    \n",
    "    # Amenity density (amenities / square kilometer)\n",
    "    amts_dens = amenities / areas_sqkm\n",
    "    \n",
    "    # Normalize densities\n",
    "    if np.max(amts_dens) > 0:\n",
    "        amts_dens /= np.max(amts_dens)\n",
    "    \n",
    "    # Display data\n",
    "    if viewData:\n",
    "        tqdm.write(f\"\\n{'[Region]':<40} {'[Area (sq km)]':>12} {'[# Amenities]':>10}\")\n",
    "        tqdm.write(\"\\n\".join(data_output))\n",
    "    \n",
    "    return amts_dens\n",
    "\n",
    "\n",
    "# Distances Between Centroids Calculation\n",
    "def compute_centroid_distances():\n",
    "    n = len(centroids)\n",
    "    \n",
    "    # Map centroids to nearest node\n",
    "    centroid_nodes = [ox.nearest_nodes(g, c[1], c[0]) for c in centroids]\n",
    "    # 2D matrix of centroid-to-centroid distance\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Loop through each pair of nodes\n",
    "    for i in tqdm(range(n), desc=\"\\nComputing centroid distances\"):\n",
    "        source_node = centroid_nodes[i]\n",
    "        for j in range(i, n):\n",
    "            target_node = centroid_nodes[j]\n",
    "            \n",
    "            # Retrieve distance between source and target centroid nodes\n",
    "            if (i == j):\n",
    "                distance = 0\n",
    "            else :\n",
    "                distance = nx.shortest_path_length(g, source=source_node, target=target_node, weight='length')\n",
    "                distance_matrix[i][j] = distance\n",
    "                distance_matrix[j][i] = distance #symmetrix matrix\n",
    "    \n",
    "    # Normalize and return\n",
    "    if distance_matrix.max() > 0:\n",
    "        distance_matrix  /= distance_matrix.max()\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "#compute density of amenities\n",
    "amts_dens = compute_amts_dens()\n",
    "        \n",
    "# Initialize dictionary of distances between centroids\n",
    "centroid_distances = compute_centroid_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T01:53:13.054734Z",
     "start_time": "2024-10-21T01:53:13.046237Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, i, dow, city, alpha=0.5):\n",
    "\n",
    "        ''' \n",
    "        Initialize an Agent instance\n",
    "        Parameters:\n",
    "        - i (int): Agent identifier.\n",
    "        - dow (float): Endowment value.\n",
    "        - city (City): Reference to the City instance.\n",
    "        - alpha (float): Weighting factor for transit access vs. community value.\n",
    "        '''\n",
    "        self.i = i\n",
    "        self.dow = dow\n",
    "        self.city = city \n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.weights = None\n",
    "        self.probabilities = None # Probability to go to each centroid\n",
    "        self.tot_probabilities = None\n",
    "        self.avg_probabilities = None\n",
    "        self.u = None\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "    # Create hash identifier\n",
    "    def __hash__(self):\n",
    "        return hash(self.i)\n",
    "\n",
    "    def __eq__(self, other): \n",
    "        return self.i == other.i\n",
    "\n",
    "    '''\n",
    "    - Assign starting centroid\n",
    "    - Reset centroid weights/probabilities\n",
    "    '''\n",
    "    def reset(self):\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.ones(len(centroids))\n",
    "        \n",
    "        # Initialize probabilities\n",
    "        self.probabilities = np.array(self.weights / self.weights.sum())\n",
    "        \n",
    "        # Initialize tot_probabilities\n",
    "        self.tot_probabilities = self.probabilities\n",
    "        \n",
    "        # Initialize current node - Initialize starting position at random node (based on weights)\n",
    "        self.u = np.random.choice(self.city.n, p=self.probabilities) \n",
    "        \n",
    "        # Adds self to the current node's set of inhabitants in inh_array\n",
    "        self.city.inh_array[self.u].add(self)\n",
    "\n",
    "# ACTION METHOD\n",
    "    def act(self): \n",
    "        # Leave node\n",
    "        self.city.inh_array[self.u].remove(self)\n",
    "    \n",
    "        # Choose another node\n",
    "        self.u = np.random.choice(self.city.n, p=self.probabilities) \n",
    "    \n",
    "        # Join node\n",
    "        self.city.inh_array[self.u].add(self)\n",
    "        \n",
    "# LEARN METHOD\n",
    "    def learn(self):\n",
    "        costs = self.cost_vector()\n",
    "        self.weights *= (1 - EPSILON * costs) # array of weights, based on COST\n",
    "        self.probabilities = self.weights / np.sum(self.weights) #normalize\n",
    "        self.tot_probabilities += self.probabilities\n",
    "        \n",
    "    def cost_vector(self):\n",
    "        dow_thr = self.city.dow_thr_array\n",
    "        \n",
    "        aff = (self.dow >= dow_thr).astype(float) # Affordability: Is the agent's endowment >= endowment threshold?\n",
    "        loc = centroid_distances[self.u, :] # Location cost: Distances from centroid to all other centroids\n",
    "        cmt = np.exp(- self.alpha * np.abs(self.dow - self.city.cmt_array)) # Community cost\n",
    "        acc = np.exp(- (1 - self.alpha) * amts_dens) # Accessibility cost\n",
    "        upk = self.city.upk_array # Upkeep cost\n",
    "        beltline = self.city.beltline_array # Beltline cost\n",
    "        \n",
    "        cost = 1 - aff * upk * beltline * loc * cmt * acc\n",
    "        return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T01:53:12.249571Z",
     "start_time": "2024-10-21T01:53:12.219247Z"
    }
   },
   "outputs": [],
   "source": [
    "class City:\n",
    "\n",
    "    # CONSTRUCTOR\n",
    "    def __init__(self, centroids, g, rho=2): #default rho (house capacity) == 2\n",
    "        '''\n",
    "        Initialize a City instance.\n",
    "        '''\n",
    "        self.rho = rho #house capacity\n",
    "        self.centroids = centroids #centroids list\n",
    "        self.g = g #OSMnx map\n",
    "        self.n = len(centroids)\n",
    "        \n",
    "        # STORE ATTRIBUTES OF ALL CENTROIDS \n",
    "        self.lat_array = np.array([lat for lat, _, _, _ in centroids]) # Latitude\n",
    "        self.lon_array = np.array([lon for _, lon, _, _ in centroids]) # Longitude\n",
    "        self.beltline_array = np.array([beltline for _, _, _, beltline in centroids], dtype=bool).astype(float) # In Beltline?\n",
    "        self.name_array = [name for _, _, name, _ in centroids] # Centroid region name\n",
    "        \n",
    "        self.inh_array = [set() for _ in range(self.n)] # Array of sets - each set contains Agent inhabitants\n",
    "        self.dow_thr_array = np.zeros(self.n) # Endowment threshold\n",
    "        self.upk_array = np.zeros(self.n, dtype=bool) # Upkeep score\n",
    "        self.cmt_array = np.zeros(self.n) # Community score\n",
    "        \n",
    "        self.pop_hist = [[] for _ in range(self.n)]  # Population history - list of lists \n",
    "        self.cmt_hist = [[] for _ in range(self.n)]  # Community score history - list of lists \n",
    "        \n",
    "        self.node_array = np.array([ox.nearest_nodes(self.g, lon, lat) for lon, lat in zip(self.lon_array, self.lat_array)])\n",
    "\n",
    "    # Set agents and their endowments\n",
    "    def set_agts(self, agts):\n",
    "        self.agts = agts #list of agents\n",
    "        self.agt_dows = np.array([a.dow for a in self.agts]) #array of agent endowments\n",
    "\n",
    "    # Update each node (cmt score, population)\n",
    "    def update(self):   \n",
    "        for ID in range(self.n): # For each centroid\n",
    "\n",
    "            inhabitants = self.inh_array[ID]\n",
    "            \n",
    "            pop = len(inhabitants)\n",
    "            \n",
    "            # Update population history\n",
    "            self.pop_hist[ID].append(pop)\n",
    "            \n",
    "            if pop > 0: # Inhabited\n",
    "                #UPDATE COMMUNITY SCORE (avg inhabitant dows, weighted by distance to other centroids)\n",
    "                inhabitant_dows = np.array([a.dow for a in self.inh_array[ID]])  # Array of endowments of node's inhabitants\n",
    "                distances = centroid_distances[ID, [a.u for a in self.inh_array[ID]]]\n",
    "                weights = (1 - distances) ** 2\n",
    "                \n",
    "                # Update Community history (average endowment)\n",
    "                cmt = np.average(inhabitant_dows, weights=weights) \n",
    "                \n",
    "                # Establish endowment threshold\n",
    "                if pop < self.rho:\n",
    "                    self.dow_thr_array[ID] = 0.0\n",
    "                else:\n",
    "                    self.dow_thr_array[ID] = np.partition(inhabitant_dows, -self.rho)[-self.rho]\n",
    "                self.upk_array[ID] = True\n",
    "                \n",
    "            else: # If uninhabited\n",
    "                self.dow_thr_array[ID] = 0.0\n",
    "                self.upk_array[ID] = False\n",
    "                cmt = 0.0\n",
    "\n",
    "            # Update Community history (average endowment)\n",
    "            self.cmt_hist[ID].append(cmt)\n",
    "            \n",
    "    \n",
    "    # =====================\n",
    "    # SAVE DATA TO CSV FILE\n",
    "    # =====================\n",
    "    def get_data(self):\n",
    "        data = [] # Array storing data for each centroid\n",
    "        \n",
    "        for ID in range(self.n):\n",
    "            # Name\n",
    "            centroid_name = self.name_array[ID]\n",
    "            \n",
    "            # Population\n",
    "            population = len(self.inh_array[ID])\n",
    "            \n",
    "            # Average Endowment\n",
    "            if population > 0:\n",
    "                avg_endowment = 100 * (np.mean([agent.dow for agent in self.inh_array[ID]]))\n",
    "            else:\n",
    "                avg_endowment = 0.0\n",
    "                \n",
    "            # In Beltline?\n",
    "            in_beltline = self.beltline_array[ID]\n",
    "            \n",
    "            # Amenity Density\n",
    "            amenity_density = amts_dens[ID]\n",
    "\n",
    "            data.append({\n",
    "                'Centroid': centroid_name,\n",
    "                'Population': population,\n",
    "                'Avg Endowment': round(avg_endowment, 2),\n",
    "                'In Beltline': in_beltline,\n",
    "                'Amt Density': round(amenity_density, 2)\n",
    "            })\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "        \n",
    "    # ==================\n",
    "    # CITY PLOTTING CODE\n",
    "    # ==================\n",
    "    def plot(self, cmap='YlOrRd', figkey='city', graph=None):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "        if graph:\n",
    "            ox.plot_graph(graph, ax=ax, node_color='black', node_size=10, edge_color='gray', edge_linewidth=1, show=False, close=False)\n",
    "    \n",
    "        # Prepare agent data\n",
    "        agent_lats = np.array([self.lat_array[agent.u] for agent in self.agts])\n",
    "        agent_lons = np.array([self.lon_array[agent.u] for agent in self.agts])\n",
    "        agent_wealths = np.array([agent.dow for agent in self.agts])\n",
    "    \n",
    "        # Population density heatmap\n",
    "        heatmap, xedges, yedges = np.histogram2d(agent_lons, agent_lats, bins=30)\n",
    "        extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "        ax.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, alpha=0.5)\n",
    "    \n",
    "        # Plot agents with wealth-based marker sizes\n",
    "        norm = Normalize(vmin=agent_wealths.min(), vmax=agent_wealths.max())\n",
    "        marker_sizes = [50 + 150 * norm(agent_wealths)]\n",
    "        sc = ax.scatter(agent_lons, agent_lats, c=agent_wealths, s=marker_sizes, cmap='coolwarm', alpha=0.7, edgecolor='red')\n",
    "    \n",
    "        # Plot centroids locations (this comes after the graph to make sure they are visible on top)\n",
    "        \n",
    "        colors = np.where(self.beltline_array, 'yellow', 'white')\n",
    "        ax.scatter(self.lon_array, self.lat_array, color=colors, s=100, alpha=0.7, edgecolor='black')\n",
    "            \n",
    "        for ID in range(self.n):    \n",
    "            lon = self.lon_array[ID]\n",
    "            lat = self.lat_array[ID]\n",
    "            # Display inhabitant populations at each node:\n",
    "            inhabitants = len(self.inh_array[ID])\n",
    "            ax.text(lon, lat, str(inhabitants), fontsize=9, ha='center', va='center', color='black')\n",
    "\n",
    "    \n",
    "        # Add color bar for wealth\n",
    "        cbar = plt.colorbar(sc, ax=ax, orientation='vertical', label='Wealth (dow)')\n",
    "    \n",
    "        # Labels and title\n",
    "        ax.set_title(f\"City Visualization: {figkey}\")\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "    \n",
    "        # Legend\n",
    "        ax.scatter([], [], c='yellow', s=100, label='Beltline Housing')\n",
    "        ax.scatter([], [], c='white', s=100, label='Non-Beltline Housing')\n",
    "        ax.scatter([], [], c='red', s=100, label='Agents')\n",
    "        ax.legend(loc='upper right')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./figures/{figkey}.pdf', format='pdf', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===============\n",
    "# # FOUR-STEP MODEL\n",
    "# # ===============\n",
    "\n",
    "# # SUPPLY data (transportation network):\n",
    "# url = 'https://tigerweb.geo.census.gov/arcgis/rest/services/TIGERweb/State_County/MapServer/37/query?where=state%3D06&f=geojson'\n",
    "# r = requests.get(url)\n",
    "# zones = geopandas.GeoDataFrame.from_features(r.json()['features'])\n",
    "# centroidFunction = lambda row: (row['geometry'].centroid.y, row['geometry'].centroid.x)\n",
    "# zones['centroid'] = zones.apply(centroidFunction, axis=1)\n",
    "\n",
    "# # DEMAND data (transportation network users):\n",
    "# url = 'http://api.census.gov/data/2015/acs5/profile?get=NAME,DP03_0018E&for=county&in=state:06'\n",
    "# r = requests.get(url)\n",
    "# Production = pandas.DataFrame(r.json()[1:], columns = r.json()[0], dtype='int')\n",
    "# nameSplit = lambda x: x.split(',')[0]\n",
    "# Production['NAME'] = Production['NAME'].apply(nameSplit)\n",
    "# zones = pandas.merge(zones, Production)\n",
    "# zones['Production'] = zones['DP03_0018E']\n",
    "\n",
    "# def getEmployment(state, county):\n",
    "#     prefix = 'EN'\n",
    "#     seasonal_adjustment = 'U'\n",
    "#     area = format(state, \"02d\") + format(county, \"03d\")\n",
    "#     data_type = '1'\n",
    "#     size = '0'\n",
    "#     ownership = '0'\n",
    "#     industry = '10'\n",
    "#     seriesid = prefix + seasonal_adjustment + area + data_type + size + ownership + industry\n",
    "#     headers = {'Content-type': 'application/json'}\n",
    "#     data = json.dumps({\"seriesid\": [seriesid],\"startyear\":\"2015\", \"endyear\":\"2015\", \"registrationKey\": \"\"})\n",
    "#     p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "#     employment = p.json()['Results']['series'][0]['data'][0]['value']\n",
    "#     return(employment)\n",
    "\n",
    "# employment = lambda row: int(getEmployment(row['state'], row['county']))\n",
    "# zones['Attraction'] = zones.transpose().apply(employment)\n",
    "# zones['Production'] = zones['Production'] * zones.sum()['Attraction'] / zones.sum()['Production']\n",
    "# zones.index = zones.NAME\n",
    "# zones.sort_index(inplace=True)\n",
    "\n",
    "# # TRIP DISTRIBUTION:\n",
    "# def costFunction(zones, zone1, zone2, beta):\n",
    "#     cost = math.exp(-beta * haversine(zones[zone1]['centroid'], zones[zone2]['centroid']))\n",
    "#     return(cost)\n",
    "\n",
    "# def costMatrixGenerator(zones, costFunction, beta):\n",
    "#     originList = []\n",
    "#     for originZone in zones:\n",
    "#         destinationList = []\n",
    "#         for destinationZone in zones:\n",
    "#             destinationList.append(costFunction(zones, originZone, destinationZone, beta))\n",
    "#         originList.append(destinationList)\n",
    "#     return(pandas.DataFrame(originList, index=zones.columns, columns=zones.columns))\n",
    "\n",
    "# def tripDistribution(tripGeneration, costMatrix):\n",
    "#     costMatrix['ozone'] = costMatrix.columns\n",
    "#     costMatrix = costMatrix.melt(id_vars=['ozone'])\n",
    "#     costMatrix.columns = ['ozone', 'dzone', 'total']\n",
    "#     production = tripGeneration['Production']\n",
    "#     production.index.name = 'ozone'\n",
    "#     attraction = tripGeneration['Attraction']\n",
    "#     attraction.index.name = 'dzone'\n",
    "#     aggregates = [production, attraction]\n",
    "#     dimensions = [['ozone'], ['dzone']]\n",
    "#     IPF = ipfn.ipfn(costMatrix, aggregates, dimensions)\n",
    "#     trips = IPF.iteration()\n",
    "#     return(trips.pivot(index='ozone', columns='dzone', values='total'))\n",
    "\n",
    "# beta = 0.01\n",
    "# costMatrix = costMatrixGenerator(zones.transpose(), costFunction, beta)\n",
    "# trips = tripDistribution(zones, costMatrix)\n",
    "\n",
    "# # MODE CHOICE:\n",
    "# def modeChoiceFunction(zones, zone1, zone2, modes):\n",
    "#     distance = haversine(zones[zone1]['centroid'], zones[zone2]['centroid'])\n",
    "#     probability = {}\n",
    "#     total = 0.0\n",
    "#     for mode in modes:\n",
    "#         total = total + math.exp(modes[mode] * distance)\n",
    "#     for mode in modes:\n",
    "#         probability[mode] = math.exp(modes[mode] * distance) / total\n",
    "#     return(probability)\n",
    "\n",
    "# def probabilityMatrixGenerator (zones, modeChoiceFunction, modes):\n",
    "#     probabilityMatrix = {}\n",
    "#     for mode in modes:\n",
    "#         originList = []\n",
    "#         for originZone in zones:\n",
    "#             destinationList = []\n",
    "#             for destinationZone in zones:\n",
    "#                 destinationList.append(modeChoiceFunction(zones, originZone, destinationZone, modes)[mode])\n",
    "#             originList.append(destinationList)\n",
    "#         probabilityMatrix[mode] = pandas.DataFrame(originList, index=zones.columns, columns=zones.columns)\n",
    "#     return(probabilityMatrix)\n",
    "\n",
    "# modes = {'walking': .05, 'cycling': .05, 'driving': .05}\n",
    "# probabilityMatrix = probabilityMatrixGenerator(zones.transpose(), modeChoiceFunction, modes)\n",
    "# drivingTrips = trips * probabilityMatrix['driving']\n",
    "\n",
    "# #ROUTE ASSIGNMENT:\n",
    "# def routeAssignment(zones, trips):\n",
    "#     G = networkx.Graph()\n",
    "#     G.add_nodes_from(zones.columns)\n",
    "#     for zone1 in zones:\n",
    "#         for zone2 in zones:\n",
    "#             if zones[zone1]['geometry'].touches(zones[zone2]['geometry']):\n",
    "#                 G.add_edge(zone1, zone2, distance = haversine(zones[zone1]['centroid'], zones[zone2]['centroid']), volume=0.0)\n",
    "#     for origin in trips:\n",
    "#         for destination in trips:\n",
    "#             path = networkx.shortest_path(G, origin, destination)\n",
    "#             for i in range(len(path) - 1):\n",
    "#                 G[path[i]][path[i + 1]]['volume'] = G[path[i]][path[i + 1]]['volume'] + trips[zone1][zone2]\n",
    "#     return(G)\n",
    "\n",
    "# def visualize(G, zones):\n",
    "#     fig = pyplot.figure(1, figsize=(10, 10), dpi=90)\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     zonesT = zones.transpose()\n",
    "#     zonesT.plot(ax = ax)\n",
    "#     for i, row in zones.transpose().iterrows():\n",
    "#         text = pyplot.annotate(s=row['NAME'], xy=((row['centroid'][1], row['centroid'][0])), horizontalalignment='center', fontsize=6)\n",
    "#         text.set_path_effects([patheffects.Stroke(linewidth=3, foreground='white'), patheffects.Normal()])\n",
    "#     for zone1 in G.edge:\n",
    "#         for zone2 in G.edge[zone1]:\n",
    "#             volume = G.edge[zone1][zone2]['volume']\n",
    "#             x = [zones[zone1]['centroid'][1], zones[zone2]['centroid'][1]]\n",
    "#             y = [zones[zone1]['centroid'][0], zones[zone2]['centroid'][0]]\n",
    "#             ax.plot(x, y, color='#444444', linewidth=volume/10000, solid_capstyle='round', zorder=1)\n",
    "#     pyplot.show(block=False)\n",
    "\n",
    "# G = routeAssignment(zones.transpose(), drivingTrips)\n",
    "# visualize(G, zones.transpose())\n",
    "\n",
    "# # TO PLAY AROUND WITH PARAMETERS (LIST OF PARAMETERS:)\n",
    "# # Trip Distribution\n",
    "#     #beta = 0.01\n",
    "#     #costMatrix = costMatrixGenerator(zones.transpose(), costFunction, beta)\n",
    "#     #trips = tripDistribution(zones, costMatrix)\n",
    "# # Mode Choice\n",
    "#     #modes = {'walking': .05, 'cycling': .05, 'driving': .05}\n",
    "#     #probabilityMatrix = probabilityMatrixGenerator(zones.transpose(), modeChoiceFunction, modes)\n",
    "#     #drivingTrips = trips * probabilityMatrix['driving']\n",
    "# # Route Assignment\n",
    "#     #G = routeAssignment(zones.transpose(), drivingTrips)\n",
    "#     #visualize(G, zones.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T01:50:53.060468Z",
     "start_time": "2024-10-21T01:50:40.048149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating:: 100%|██████████| 999/999 [00:05<00:00, 178.85step/s, alpha=0.25, rho=4, timestep=998]\n"
     ]
    }
   ],
   "source": [
    "# ==========\n",
    "# SIMULATION \n",
    "# ==========\n",
    "\n",
    "# =========================\n",
    "# PRE-DETERMINED PARAMETERS\n",
    "# =========================\n",
    "rho_l = [4] #1, 2, 4, 8 (for each iteration) rho-house capacity\n",
    "alpha_l = [0.25] #0.25, 0.75 (for each iteration) lambda - centroid proximity vs. community value\n",
    "t_max_l = [999] #5000, 10000, 15000, 20000 (for each iteration) timesteps\n",
    "\n",
    "tau = 0.5 # Inequality factor in Lorentz curve\n",
    "num_agents = 150 # Number of agents\n",
    "\n",
    "# RUN SIMULATION?\n",
    "run_experiments = True\n",
    "\n",
    "# PLOT SIMULATION?\n",
    "plot_cities = False\n",
    "\n",
    "# SAVE POPULATION/ENDOWMENT DATA TO CSV'S?\n",
    "save_data = True\n",
    "\n",
    "# City key (name)\n",
    "cty_key = 'Georgia'\n",
    "\n",
    "# ===============\n",
    "# SIMULATION CODE\n",
    "# ===============\n",
    "if run_experiments:\n",
    "    # Number of rho/alpha combinations\n",
    "    rho_alpha_iterations = len(rho_l) * len(alpha_l) * max(t_max_l)\n",
    "    with tqdm(total=rho_alpha_iterations, desc='Simulating:', unit = 'step') as pbar:\n",
    "        for rho, alpha in itertools.product(rho_l, alpha_l):\n",
    "            # Ensure reproducibility \n",
    "            np.random.seed(0)\n",
    "\n",
    "            # Initialize city and agents\n",
    "            city = City(centroids, g, rho=rho)\n",
    "            agt_dows = np.diff([1 - (1 - x) ** tau for x in np.linspace(0, 1, num_agents + 1)]) \n",
    "            agts = [Agent(i, dow, city, alpha=alpha) for i, dow in enumerate(agt_dows)]\n",
    "\n",
    "            city.set_agts(agts)\n",
    "            city.update()\n",
    "            \n",
    "            # Iterate through t_max_l\n",
    "            for t in range(max(t_max_l)):\n",
    "                pbar.set_postfix(rho=rho, alpha=alpha, timestep=t)\n",
    "                for a in agts:\n",
    "                    a.act()\n",
    "                city.update()\n",
    "                for a in agts:\n",
    "                    a.learn()\n",
    "                    \n",
    "                pbar.update(1)\n",
    "                \n",
    "                if (t + 1) in t_max_l:\n",
    "                    for a in city.agts:\n",
    "                        a.avg_probabilities = a.tot_probabilities / (t + 1)\n",
    "\n",
    "                    # Pickle city object\n",
    "                    with open(Path(cwd / 'data/{0}_{1}_{2}_{3}.pkl'.format(cty_key, rho, alpha, t + 1)), 'wb') as file:\n",
    "                        pickle.dump(city, file)\n",
    "                    \n",
    "                    # ==================================\n",
    "                    # CENTROID DATA TO CSV via DATAFRAME\n",
    "                    # ==================================\n",
    "                    if save_data == True:\n",
    "                        df_data = city.get_data()\n",
    "\n",
    "                        # CSV filename\n",
    "                        csv_filename = f\"{cty_key}_{rho}_{alpha}_{t + 1}_data.csv\"\n",
    "                        \n",
    "                        # Save dataframe to CSV file\n",
    "                        csv_path = Path(cwd / 'data' / csv_filename)\n",
    "                        df_data.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "if plot_cities:\n",
    "    for rho in rho_l:\n",
    "        for alpha in alpha_l:\n",
    "            for t_max in t_max_l:\n",
    "                with open(Path(cwd / 'data/{0}_{1}_{2}_{3}.pkl'.format(cty_key, rho, alpha, t_max)), 'rb') as file:\n",
    "                    city = pickle.load(file)\n",
    "                cmap = 'YlOrRd'\n",
    "                figkey = '{0}_{1}_{2}_{3}'.format(cty_key, rho, alpha, t_max)\n",
    "                city.plot(cmap=cmap, figkey=figkey, graph=g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
